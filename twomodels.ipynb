{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Here are the reference websites used in this code:\n\n#FOR WORD2VEC:\n#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n\n#FOR BAYESIAN-OPTIMIZATION:\n#https://medium.com/@crawftv/parameter-hyperparameter-tuning-with-bayesian-optimization-7acf42d348e1\n\n#FOR BERT:\n#https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=ex5O1eV-Pfct\n#https://www.kaggle.com/sharmilaupadhyaya/20newsgroup-classification-using-keras-bert-in-gpu\n\n#FOR METRICS:\n#https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/plot_precision_recall.html\n\n#FOR TPU USAGE:\n#https://www.kaggle.com/docs/tpu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Siamese LSTM Neural Network"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import modules:\n\nimport pandas as pd\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras import layers\nfrom keras import Input\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import to_categorical\nfrom bayes_opt import BayesianOptimization\nfrom tensorflow.python.keras.optimizer_v2 import rmsprop\nfrom skopt import gbrt_minimize, gp_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.space import Real, Categorical, Integer\nimport tensorflow as tf\nfrom keras import backend as K\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom skopt.plots import plot_convergence\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import average_precision_score\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom keras.utils.vis_utils import plot_model\nimport pylab as pl\nimport warnings\nwarnings.filterwarnings('ignore')\nimport io\nimport random\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom gluonnlp.calibration import BertLayerCollector\nimport sys\nimport numpy as np","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clear session storage:\nK.clear_session()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run this if you use TPU:\n\n# detect and init the TPU:\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n    \n# instantiate a distribution strategy:\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load datasets:\ntrain = pd.read_csv(\"../input/quora-data/quora_train.csv\")\ntest = pd.read_csv(\"../input/quora-data/quora_test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#handle missing values:\n#q1: 1; q2: 1\n#train.isna().sum() \n\n#q1: 0; q2: 1\n#test.isna().sum()\n\ntrain.dropna(axis = 0, how = \"any\", inplace = True)\ntest.dropna(axis = 0, how = \"any\", inplace = True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the datasets and split the training data into train_set and validation_set:\ntrain_set = train.sample(frac = 0.8, random_state = 111)\nind_train = train_set.index\nvalidation_set = train.drop(ind_train, axis = 0)\ntest = test.sample(frac = 1)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data for train set:\nmax_words = 20000\n\nquestion_list_train = train_set.question1.tolist() + train_set.question2.tolist()\ntokenizer = Tokenizer(num_words = max_words,\n                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                      lower = True)\n\ntokenizer.fit_on_texts(question_list_train)\nword_index = tokenizer.word_index\nsequences_train = tokenizer.texts_to_sequences(question_list_train)\nX_train = pad_sequences(sequences_train)\n\ntrain_q1 = X_train[:train_set.shape[0], :]\ntrain_q2 = X_train[train_set.shape[0]:, :]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data for validation set:\nsequences_validation_q1 = tokenizer.texts_to_sequences(validation_set.question1)\nsequences_validation_q2 = tokenizer.texts_to_sequences(validation_set.question2)\nvalidation_q1 = pad_sequences(sequences_validation_q1, maxlen = X_train.shape[1])\nvalidation_q2 = pad_sequences(sequences_validation_q2, maxlen = X_train.shape[1])\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess for the whole training data\nquestion_list_training = train.question1.tolist() + train.question2.tolist()\ntokenizer_whole = Tokenizer(num_words = max_words,\n                            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                            lower = True)\n\ntokenizer_whole.fit_on_texts(question_list_training)\nsequences_training = tokenizer_whole.texts_to_sequences(question_list_training)\nX_training = pad_sequences(sequences_training, maxlen = X_train.shape[1])\n\ntraining_q1 = X_training[:train.shape[0], :]\ntraining_q2 = X_training[train.shape[0]:, :]\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data for test set:\nsequences_test_q1 = tokenizer_whole.texts_to_sequences(test.question1)\nsequences_test_q2 = tokenizer_whole.texts_to_sequences(test.question2)\ntest_q1 = pad_sequences(sequences_test_q1, maxlen = X_train.shape[1])\ntest_q2 = pad_sequences(sequences_test_q2, maxlen = X_train.shape[1])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the targets for train_set, validation_set, training, test data:\ny_train_set = np.asarray(train_set.is_duplicate)\ny_validation_set = np.asarray(validation_set.is_duplicate)\ny_test = np.asarray(test.is_duplicate)\ny_train = np.asarray(train.is_duplicate)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"word2vec embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the pretrained word embedding vectors and construct a matrix:\nword_vectors = KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\n\nembedding_dim = 300\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i >= max_words:\n        continue\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n\ndel(word_vectors)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#specify parameter & hyperparameter space:\nlstm_output_dim = Integer(low = 20, high = 100, name = \"lstm_output_dim\")\nlearning_rate = Real(low = 1e-4, high = 1e-2, \n                     prior =\"log-uniform\", name = \"learning_rate\")\ndropout_rate = Real(low = 0.1, high = 0.6, name = \"dropout_rate\")\n\nparam_range = [lstm_output_dim, learning_rate, dropout_rate]\ndefault_param = [64, 1e-3, 0.5]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function to get model structure before training:\ndef create_model_word2vec(lstm_output_dim, learning_rate, dropout_rate):\n    \n    maxlen = X_train.shape[1]\n    embedding_dim = 300\n    \n    input1 = Input(shape = (maxlen,), name = \"input1\")\n    input2 = Input(shape = (maxlen,), name = \"input2\")\n        \n    embedding = layers.Embedding(max_words, embedding_dim, \n                                 weights = [embedding_matrix],\n                                 input_length = maxlen, \n                                 trainable = True)\n    lstm = layers.LSTM(lstm_output_dim)\n    dropout = layers.Dropout(dropout_rate)\n        \n    embedded_output1 = embedding(input1)\n    embedded_output2 = embedding(input2)\n        \n    lstm_output1 = lstm(embedded_output1)\n    lstm_output2 = lstm(embedded_output2)\n        \n    merged = layers.concatenate([lstm_output1, lstm_output2], axis = -1)\n    dropout_output = dropout(merged)\n    predictions = layers.Dense(1, activation = \"sigmoid\")(dropout_output)\n    \n    model = Model([input1, input2], predictions)\n    optimizer = rmsprop.RMSProp(learning_rate = learning_rate)\n    model.compile(optimizer = optimizer, \n                  loss = \"binary_crossentropy\", \n                  metrics = [tf.keras.metrics.AUC()])\n        \n    return(model)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function to do model fitting:\n@use_named_args(dimensions = param_range)\ndef fitness_word2vec(lstm_output_dim, learning_rate, dropout_rate):\n    \n    model = create_model_word2vec(lstm_output_dim, learning_rate, dropout_rate)\n    model.fit([train_q1, train_q2], y_train_set, epochs = 2, \n              batch_size = 128)\n    \n    ypred = model.predict([validation_q1, validation_q2])\n    score = average_precision_score(y_validation_set, ypred)\n        \n    del model\n    K.clear_session()\n    tf.compat.v1.reset_default_graph()\n        \n    return(-score)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gaussian Processï¼š\ngp_word2vec_result = gp_minimize(func = fitness_word2vec,\n                                 dimensions = param_range,\n                                 n_calls = 20,\n                                 n_jobs = -1,\n                                 kappa = 5,\n                                 x0 = default_param)","execution_count":15,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n1856/2022 [==========================>...] - ETA: 14s - loss: 0.5320 - auc: 0.7817","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-256f3e0c11f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                  \u001b[0mkappa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                  x0 = default_param)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# evaluate y0 if only x0 is provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mn_calls\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m# record through tell function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-186301f25975>\u001b[0m in \u001b[0;36mfitness_word2vec\u001b[0;34m(lstm_output_dim, learning_rate, dropout_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     model.fit([train_q1, train_q2], y_train_set, epochs = 2, \n\u001b[0;32m----> 7\u001b[0;31m               batch_size = 128)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_q2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get convergence plot:\nplot_convergence(gp_word2vec_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print out the optimum parameters & hyperparameters:\nprint(\"lstm_output_dim\": gp_word2vec_result.x[0], \n      \"learning_rate\": gp_word2vec_result.x[1], \n      \"dropout_rate\": gp_word2vec_result.x[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print out the optimal objective function value:\ngp_word2vec_result.fun","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#epoch = 2, n_calls = 20:\n#build and re-train the model on the whole training data (run this code on TPU):\nmodel_word2vec = create_model_word2vec(100, 0.003458554720057225, 0.1)\nmodel_word2vec.fit([training_q1, training_q2], y_train, epochs = 2, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get predicted probabilities on test set:\ntest_pred = model_word2vec.predict([test_q1, test_q2])\ntest_score = average_precision_score(y_test, test_pred)\ntest_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show model archetecture:\nplot_model(model_word2vec, show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show model details:\nmodel_word2vec.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Precision-Recall and plot curve\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred)\narea = auc(recall, precision)\nprint(\"Area Under Curve: %0.2f\" % area)\n\npl.clf()\npl.plot(recall, precision, label='Precision-Recall curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title('Precision-Recall Curve for Siamese Model with Word2Vec Embeddings: AUC=%0.2f' % area)\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete the precision and recall without corresponding threshold:\nprecision = precision[:precision.shape[0] - 1]\nrecall = recall[:recall.shape[0] - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#precision-thresholds curve:\npl.clf()\npl.plot(thresholds, precision, label='Precision-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Precision-Thresholds Curve for Siamese Model with Word2Vec Embeddings\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recall-thresholds curve:\npl.clf()\npl.plot(thresholds, recall, label='Recall-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Recall')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Recall-Thresholds Curve for Siamese Model with Word2Vec Embeddings\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the predicted class labels:\ntest_pred_classes = []\nfor i in range(test_pred.shape[0]):\n    if test_pred[i] >= 0.5:\n        test_pred_classes.append(1)\n    else:\n        test_pred_classes.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print out the classification report:\nprint(classification_report(y_test, test_pred_classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine Tuning with BERT "},{"metadata":{"trusted":true},"cell_type":"code","source":"#install packages and download zip file:\n!pip install keras-bert\n!pip install keras-rectified-adam\n!pip install h5py\n\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import relavant modules:\nimport os\nimport codecs\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom chardet import detect\nimport keras\nfrom keras_radam import RAdam\nfrom keras import backend as K\nfrom keras_bert import load_trained_model_from_checkpoint\nfrom keras_bert import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameters;\nSEQ_LEN = 128\nBATCH_SIZE = 50\nEPOCHS = 1\nLR = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Path to get the pre trained model of BERT.\npretrained_path = \"../input/output/uncased_L-12_H-768_A-12\"\nconfig_path = \"./uncased_L-12_H-768_A-12/bert_config.json\"\ncheckpoint_path = \"./uncased_L-12_H-768_A-12/bert_model.ckpt\"\nvocab_path = \"./uncased_L-12_H-768_A-12/vocab.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Pretrained BERT model:\nwith tpu_strategy.scope():\n    model = load_trained_model_from_checkpoint(\n        config_path,\n        checkpoint_path,\n        training = True,\n        trainable = True,\n        seq_len = SEQ_LEN,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the details of original BERT model:\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show the archetecture of the original BERT model:\nplot_model(model, show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the transformer for BERT:\n!pip install transformers\nfrom transformers import BertTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n    \n    sentences_train = train.question1 + \" [SEP] \" + train.question2\n    sentences_train_values = sentences_train.values\n    labels_train = train.is_duplicate.values\n    \n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    token_ids_train = []\n    attention_masks_train = []\n    \n    # For every sentence in training data:\n    for sent in sentences_train_values:\n        encoded_dict = tokenizer.encode_plus(sent,                      \n                                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                             max_length = SEQ_LEN,      # Pad & truncate all sentences.\n                                             pad_to_max_length = True,\n                                             return_attention_mask = True,   # Construct attn. masks.\n                                             return_tensors = 'tf'           # Return pytorch tensors.\n                                            )\n        \n        # Add the encoded sentence to the list.    \n        token_ids_train.append(encoded_dict['input_ids'])\n        \n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks_train.append(encoded_dict['attention_mask'])\n    \n    # Convert the lists into tensors.\n    token_ids_train_tensor = tf.concat(token_ids_train, 0)\n    attention_masks_train_tensor = tf.concat(attention_masks_train, 0)\n    labels_train_tensor = tf.convert_to_tensor(labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    token_type_ids_train_list = []\n    \n    for i in train.index:\n        sentence_1 = train.question1[i]\n        sentence_2 = train.question2[i]\n        \n        token_ids_train_s1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_1))\n        token_ids_train_s2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_2))\n        \n        token_type_ids_train = tokenizer.create_token_type_ids_from_sequences(token_ids_train_s1, token_ids_train_s2)\n        token_type_ids_train_list.append(token_type_ids_train)\n    \n    token_type_ids_train_tensor = tf.convert_to_tensor(pad_sequences(token_type_ids_train_list, maxlen = SEQ_LEN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    sentences_test = test.question1 + \" [SEP] \" + test.question2\n    sentences_test_values = sentences_test.values\n    labels_test = test.is_duplicate.values\n    \n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    token_ids_test = []\n    attention_masks_test = []\n    \n    # For every sentence in training data:\n    for sent in sentences_test_values:\n        encoded_dict = tokenizer.encode_plus(sent,                      # Sentence to encode.\n                                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                             max_length = SEQ_LEN,           # Pad & truncate all sentences.\n                                             pad_to_max_length = True,\n                                             return_attention_mask = True,   # Construct attn. masks.\n                                             return_tensors = 'tf'     # Return pytorch tensors.\n                                            )\n        \n        # Add the encoded sentence to the list.    \n        token_ids_test.append(encoded_dict['input_ids'])\n        \n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks_test.append(encoded_dict['attention_mask'])\n        \n    # Convert the lists into tensors.\n    token_ids_test_tensor = tf.concat(token_ids_test, 0)\n    attention_masks_test_tensor = tf.concat(attention_masks_test, 0)\n    labels_test_tensor = tf.convert_to_tensor(labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    token_type_ids_test_list = []\n    \n    for i in test.index:\n        sentence_1 = test.question1[i]\n        sentence_2 = test.question2[i]\n        \n        token_ids_test_s1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_1))\n        token_ids_test_s2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_2))\n        \n        token_type_ids_test = tokenizer.create_token_type_ids_from_sequences(token_ids_test_s1, token_ids_test_s2)\n        token_type_ids_test_list.append(token_type_ids_test)\n    \n    token_type_ids_test_tensor = tf.convert_to_tensor(pad_sequences(token_type_ids_test_list, maxlen = SEQ_LEN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    inputs = model.inputs[:2]\n    dense = model.get_layer('NSP-Dense').output\n    outputs = keras.layers.Dense(units = 1, activation='sigmoid')(dense)\n    \n    model = keras.models.Model(inputs, outputs)\n    model.compile(\n        rmsprop.RMSProp(learning_rate = LR),\n        loss='binary_crossentropy',\n        metrics=[tf.keras.metrics.AUC()]\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fine-tuning with BERT:\nmodel.fit(\n    [token_ids_train_tensor, token_type_ids_train_tensor],\n    labels_train_tensor,\n    epochs = EPOCHS,\n    batch_size = BATCH_SIZE\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the predicted probabilities:\ntest_pred_bert = model.predict([token_ids_test_tensor, token_type_ids_test_tensor], \n                               verbose = True)\ntest_score_bert = average_precision_score(test.is_duplicate, test_pred_bert)\ntest_score_bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Precision-Recall and plot curve for bert model:\nprecision, recall, thresholds = precision_recall_curve(test.is_duplicate, test_pred_bert)\narea = auc(recall, precision)\nprint(\"Area Under Curve: %0.2f\" % area)\n\npl.clf()\npl.plot(recall, precision, label='Precision-Recall curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title('Precision-Recall Curve for BERT Fine Tuning Model: AUC=%0.2f' % area)\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete precision and recall without corresponding threshold:\nprecision = precision[:precision.shape[0] - 1]\nrecall = recall[:recall.shape[0] - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#precision-thresholds curve:\npl.clf()\npl.plot(thresholds, precision, label='Precision-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Precision-Thresholds Curve for BERT Fine Tuning Model\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recall-thresholds curve:\npl.clf()\npl.plot(thresholds, recall, label='Recall-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Recall')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Recall-Thresholds Curve for BERT Fine Tuning Model\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the predicted class labels:\ntest_pred_bert_classes = []\nfor i in range(test_pred_bert.shape[0]):\n    if test_pred_bert[i] >= 0.5:\n        test_pred_bert_classes.append(1)\n    else:\n        test_pred_bert_classes.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print the classification report:\nprint(classification_report(test.is_duplicate, test_pred_bert_classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualize the archetecture of models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the details of BERT:\nmodel.summary()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Here are the reference websites used in this code:\n\n#FOR WORD2VEC:\n#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n\n#FOR BAYESIAN-OPTIMIZATION:\n#https://medium.com/@crawftv/parameter-hyperparameter-tuning-with-bayesian-optimization-7acf42d348e1\n\n#FOR BERT:\n#https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=ex5O1eV-Pfct\n#https://www.kaggle.com/sharmilaupadhyaya/20newsgroup-classification-using-keras-bert-in-gpu\n\n#FOR METRICS:\n#https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/plot_precision_recall.html\n\n#FOR TPU USAGE:\n#https://www.kaggle.com/docs/tpu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import modules:\n\nimport pandas as pd\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras import layers\nfrom keras import Input\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import to_categorical\nfrom bayes_opt import BayesianOptimization\nfrom tensorflow.python.keras.optimizer_v2 import rmsprop\nfrom skopt import gbrt_minimize, gp_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.space import Real, Categorical, Integer\nimport tensorflow as tf\nfrom keras import backend as K\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom skopt.plots import plot_convergence\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import average_precision_score\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom keras.utils.vis_utils import plot_model\nimport pylab as pl\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom gluonnlp.calibration import BertLayerCollector\nimport sys\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU:\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n    \n# instantiate a distribution strategy:\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load datasets:\n\ntrain = pd.read_csv(\"../input/quora-data/quora_train.csv\")\ntest = pd.read_csv(\"../input/quora-data/quora_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#handle missing values:\n\n#q1: 1; q2: 1\n#train.isna().sum() \n\n#q1: 0; q2: 1\n#test.isna().sum()\n\ntrain.dropna(axis = 0, how = \"any\", inplace = True)\ntest.dropna(axis = 0, how = \"any\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(train.is_duplicate == 0))\nprint(sum(train.is_duplicate == 1))\n\nprint(sum(test.is_duplicate == 0))\nprint(sum(test.is_duplicate == 1))\n\nprint(203851/323478)\nprint(119627/323478)\n\nprint(51191/80870)\nprint(29679/80870)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the datasets and split the training data into train_set and validation_set:\n\ntrain_set = train.sample(frac = 0.8, random_state = 111)\nind_train = train_set.index\nvalidation_set = train.drop(ind_train, axis = 0)\n\ntest = test.sample(frac = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_set.shape)\nprint(validation_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data for train set:\nmax_words = 20000\n\nquestion_list_train = train_set.question1.tolist() + train_set.question2.tolist()\ntokenizer = Tokenizer(num_words = max_words,\n                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                      lower = True)\n\ntokenizer.fit_on_texts(question_list_train)\nword_index = tokenizer.word_index\nsequences_train = tokenizer.texts_to_sequences(question_list_train)\nX_train = pad_sequences(sequences_train)\n\ntrain_q1 = X_train[:train_set.shape[0], :]\ntrain_q2 = X_train[train_set.shape[0]:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data for validation and test set:\nsequences_validation_q1 = tokenizer.texts_to_sequences(validation_set.question1)\nsequences_validation_q2 = tokenizer.texts_to_sequences(validation_set.question2)\nvalidation_q1 = pad_sequences(sequences_validation_q1, maxlen = X_train.shape[1])\nvalidation_q2 = pad_sequences(sequences_validation_q2, maxlen = X_train.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data for the whole training data:\nquestion_list_training = train.question1.tolist() + train.question2.tolist()\ntokenizer_whole = Tokenizer(num_words = max_words,\n                            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                            lower = True)\n\ntokenizer_whole.fit_on_texts(question_list_training)\nsequences_training = tokenizer_whole.texts_to_sequences(question_list_training)\nX_training = pad_sequences(sequences_training, maxlen = X_train.shape[1])\n\ntraining_q1 = X_training[:train.shape[0], :]\ntraining_q2 = X_training[train.shape[0]:, :]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences_test_q1 = tokenizer_whole.texts_to_sequences(test.question1)\nsequences_test_q2 = tokenizer_whole.texts_to_sequences(test.question2)\ntest_q1 = pad_sequences(sequences_test_q1, maxlen = X_train.shape[1])\ntest_q2 = pad_sequences(sequences_test_q2, maxlen = X_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the targets for train_set, validation_set, training, test data:\n\ny_train_set = np.asarray(train_set.is_duplicate)\ny_validation_set = np.asarray(validation_set.is_duplicate)\ny_test = np.asarray(test.is_duplicate)\ny_train = np.asarray(train.is_duplicate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"word2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the pretrained word embedding vectors and construct a matrix:\n\nword_vectors = KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\n\nembedding_dim = 300\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i >= max_words:\n        continue\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n\ndel(word_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#specify parameter & hyperparameter space:\nlstm_output_dim = Integer(low = 20, high = 100, name = \"lstm_output_dim\")\nlearning_rate = Real(low = 1e-4, high = 1e-2, \n                     prior =\"log-uniform\", name = \"learning_rate\")\ndropout_rate = Real(low = 0.1, high = 0.6, name = \"dropout_rate\")\n\nparam_range = [lstm_output_dim, learning_rate, dropout_rate]\ndefault_param = [64, 1e-3, 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function to get model structure before training:\n\ndef create_model_word2vec(lstm_output_dim, learning_rate, dropout_rate):\n    \n    maxlen = X_train.shape[1]\n    embedding_dim = 300\n    \n    input1 = Input(shape = (maxlen,), name = \"input1\")\n    input2 = Input(shape = (maxlen,), name = \"input2\")\n        \n    embedding = layers.Embedding(max_words, embedding_dim, \n                                 weights = [embedding_matrix],\n                                 input_length = maxlen, \n                                 trainable = True)\n    lstm = layers.LSTM(lstm_output_dim)\n    dropout = layers.Dropout(dropout_rate)\n        \n    embedded_output1 = embedding(input1)\n    embedded_output2 = embedding(input2)\n        \n    lstm_output1 = lstm(embedded_output1)\n    lstm_output2 = lstm(embedded_output2)\n        \n    merged = layers.concatenate([lstm_output1, lstm_output2], axis = -1)\n    dropout_output = dropout(merged)\n    predictions = layers.Dense(1, activation = \"sigmoid\")(dropout_output)\n    \n    model = Model([input1, input2], predictions)\n    optimizer = rmsprop.RMSProp(learning_rate = learning_rate)\n    model.compile(optimizer = optimizer, \n                  loss = \"binary_crossentropy\", \n                  metrics = [tf.keras.metrics.AUC()])\n        \n    return(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function to do parameter & hyperparameter tuning:\n@use_named_args(dimensions = param_range)\ndef fitness_word2vec(lstm_output_dim, learning_rate, dropout_rate):\n    \n    model = create_model_word2vec(lstm_output_dim, learning_rate, dropout_rate)\n    model.fit([train_q1, train_q2], y_train_set, epochs = 2, \n              batch_size = 128)\n    \n    ypred = model.predict([validation_q1, validation_q2])\n    score = average_precision_score(y_validation_set, ypred)\n        \n    del model\n    K.clear_session()\n    tf.compat.v1.reset_default_graph()\n        \n    return(-score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model：\ngp_word2vec_result = gp_minimize(func = fitness_word2vec,\n                                 dimensions = param_range,\n                                 n_calls = 20,\n                                 n_jobs = -1,\n                                 kappa = 5,\n                                 x0 = default_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_convergence(gp_word2vec_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"lstm_output_dim:\", gp_word2vec_result.x[0], \n      \"learning_rate:\", gp_word2vec_result.x[1], \n      \"dropout_rate:\", gp_word2vec_result.x[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_word2vec_result.fun","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_word2vec = create_model_word2vec(gp_word2vec_result.x[0], \n                                       gp_word2vec_result.x[1],\n                                       gp_word2vec_result.x[2])\nmodel_word2vec.fit([training_q1, training_q2], y_train, epochs = 2, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#epoch = 2, n_calls = 11:\n#build and retrain the model on the whole training data:\nmodel_word2vec = create_model_word2vec(64, 0.005972004303277941, 0.1356533498522459)\nmodel_word2vec.fit([training_q1, training_q2], y_train, epochs = 2, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get predicted probabilities on test set:\ntest_pred = model_word2vec.predict([test_q1, test_q2])\ntest_score = average_precision_score(y_test, test_pred)\ntest_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Precision-Recall and plot curve\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred1)\narea = auc(recall, precision)\nprint(\"Area Under Curve: %0.2f\" % area)\n\npl.clf()\npl.plot(recall, precision, label='Precision-Recall curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title('Precision-Recall Curve for Siamese Model with Word2Vec Embeddings: AUC=%0.2f' % area)\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#epoch = 2, n_calls = 20:\n#build and retrain the model on the whole training data:\nmodel_word2vec = create_model_word2vec(100, 0.003458554720057225, 0.1)\nmodel_word2vec.fit([training_q1, training_q2], y_train, epochs = 2, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get predicted probabilities on test set:\ntest_pred = model_word2vec.predict([test_q1, test_q2])\ntest_score = average_precision_score(y_test, test_pred)\ntest_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model_word2vec, show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_word2vec.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Precision-Recall and plot curve\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred)\narea = auc(recall, precision)\nprint(\"Area Under Curve: %0.2f\" % area)\n\npl.clf()\npl.plot(recall, precision, label='Precision-Recall curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title('Precision-Recall Curve for Siamese Model with Word2Vec Embeddings: AUC=%0.2f' % area)\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = precision[:precision.shape[0] - 1]\nrecall = recall[:recall.shape[0] - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#precision-thresholds curve:\npl.clf()\npl.plot(thresholds, precision, label='Precision-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Precision-Thresholds Curve for Siamese Model with Word2Vec Embeddings\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recall-thresholds curve:\npl.clf()\npl.plot(thresholds, recall, label='Recall-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Recall')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Recall-Thresholds Curve for Siamese Model with Word2Vec Embeddings\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_classes = []\nfor i in range(test_pred.shape[0]):\n    if test_pred[i] >= 0.5:\n        test_pred_classes.append(1)\n    else:\n        test_pred_classes.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, test_pred_classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine Tuning with BERT "},{"metadata":{"trusted":true},"cell_type":"code","source":"#install packages and download zip file:\n!pip install keras-bert\n!pip install keras-rectified-adam\n!pip install h5py\n\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import relavant modules:\nimport os\nimport codecs\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom chardet import detect\nimport keras\nfrom keras_radam import RAdam\nfrom keras import backend as K\nfrom keras_bert import load_trained_model_from_checkpoint\nfrom keras_bert import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameters;\nSEQ_LEN = 128\nBATCH_SIZE = 50\nEPOCHS = 1\nLR = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Path to the pre trained model of BERT.\npretrained_path = \"../input/output/uncased_L-12_H-768_A-12\"\nconfig_path = \"./uncased_L-12_H-768_A-12/bert_config.json\"\ncheckpoint_path = \"./uncased_L-12_H-768_A-12/bert_model.ckpt\"\nvocab_path = \"./uncased_L-12_H-768_A-12/vocab.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Pretrained BERT model:\nwith tpu_strategy.scope():\n    model = load_trained_model_from_checkpoint(\n        config_path,\n        checkpoint_path,\n        training = True,\n        trainable = True,\n        seq_len = SEQ_LEN,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers\nfrom transformers import BertTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n    \n    sentences_train = train.question1 + \" [SEP] \" + train.question2\n    sentences_train_values = sentences_train.values\n    labels_train = train.is_duplicate.values\n    \n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    token_ids_train = []\n    attention_masks_train = []\n    \n    # For every sentence in training data:\n    for sent in sentences_train_values:\n        encoded_dict = tokenizer.encode_plus(sent,                      \n                                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                             max_length = SEQ_LEN,      # Pad & truncate all sentences.\n                                             pad_to_max_length = True,\n                                             return_attention_mask = True,   # Construct attn. masks.\n                                             return_tensors = 'tf'           # Return pytorch tensors.\n                                            )\n        \n        # Add the encoded sentence to the list.    \n        token_ids_train.append(encoded_dict['input_ids'])\n        \n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks_train.append(encoded_dict['attention_mask'])\n    \n    # Convert the lists into tensors.\n    token_ids_train_tensor = tf.concat(token_ids_train, 0)\n    attention_masks_train_tensor = tf.concat(attention_masks_train, 0)\n    labels_train_tensor = tf.convert_to_tensor(labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    token_type_ids_train_list = []\n    \n    for i in train.index:\n        sentence_1 = train.question1[i]\n        sentence_2 = train.question2[i]\n        \n        token_ids_train_s1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_1))\n        token_ids_train_s2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_2))\n        \n        token_type_ids_train = tokenizer.create_token_type_ids_from_sequences(token_ids_train_s1, token_ids_train_s2)\n        token_type_ids_train_list.append(token_type_ids_train)\n    \n    token_type_ids_train_tensor = tf.convert_to_tensor(pad_sequences(token_type_ids_train_list, maxlen = SEQ_LEN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\ns1 = \"Do you like eating apples?\"\ns2 = \"Do you like eating bananas?\"\ns = s1 + \" [SEP] \" + s2\n\nencoded_dict = tokenizer.encode_plus(s,                      \n                                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                     max_length = 15,      # Pad & truncate all sentences.\n                                     pad_to_max_length = True,\n                                     return_attention_mask = True,   # Construct attn. masks.\n                                     return_tensors = 'tf'           # Return pytorch tensors.\n                                    )\n        \nencoded_dict[\"input_ids\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    sentences_test = test.question1 + \" [SEP] \" + test.question2\n    sentences_test_values = sentences_test.values\n    labels_test = test.is_duplicate.values\n    \n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    token_ids_test = []\n    attention_masks_test = []\n    \n    # For every sentence in training data:\n    for sent in sentences_test_values:\n        encoded_dict = tokenizer.encode_plus(sent,                      # Sentence to encode.\n                                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                             max_length = SEQ_LEN,           # Pad & truncate all sentences.\n                                             pad_to_max_length = True,\n                                             return_attention_mask = True,   # Construct attn. masks.\n                                             return_tensors = 'tf'     # Return pytorch tensors.\n                                            )\n        \n        # Add the encoded sentence to the list.    \n        token_ids_test.append(encoded_dict['input_ids'])\n        \n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks_test.append(encoded_dict['attention_mask'])\n        \n    # Convert the lists into tensors.\n    token_ids_test_tensor = tf.concat(token_ids_test, 0)\n    attention_masks_test_tensor = tf.concat(attention_masks_test, 0)\n    labels_test_tensor = tf.convert_to_tensor(labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    token_type_ids_test_list = []\n    \n    for i in test.index:\n        sentence_1 = test.question1[i]\n        sentence_2 = test.question2[i]\n        \n        token_ids_test_s1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_1))\n        token_ids_test_s2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_2))\n        \n        token_type_ids_test = tokenizer.create_token_type_ids_from_sequences(token_ids_test_s1, token_ids_test_s2)\n        token_type_ids_test_list.append(token_type_ids_test)\n    \n    token_type_ids_test_tensor = tf.convert_to_tensor(pad_sequences(token_type_ids_test_list, maxlen = SEQ_LEN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    inputs = model.inputs[:2]\n    dense = model.get_layer('NSP-Dense').output\n    outputs = keras.layers.Dense(units = 1, activation='sigmoid')(dense)\n    \n    model = keras.models.Model(inputs, outputs)\n    model.compile(\n        rmsprop.RMSProp(learning_rate = LR),\n        loss='binary_crossentropy',\n        metrics=[tf.keras.metrics.AUC()]\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(\n    [token_ids_train_tensor, token_type_ids_train_tensor],\n    labels_train_tensor,\n    epochs = EPOCHS,\n    batch_size = BATCH_SIZE\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_bert = model.predict([token_ids_test_tensor, token_type_ids_test_tensor], \n                               verbose = True)\ntest_score_bert = average_precision_score(test.is_duplicate, test_pred_bert)\ntest_score_bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Precision-Recall and plot curve for bert model:\nprecision, recall, thresholds = precision_recall_curve(test.is_duplicate, test_pred_bert)\narea = auc(recall, precision)\nprint(\"Area Under Curve: %0.2f\" % area)\n\npl.clf()\npl.plot(recall, precision, label='Precision-Recall curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title('Precision-Recall Curve for BERT Fine Tuning Model: AUC=%0.2f' % area)\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = precision[:precision.shape[0] - 1]\nrecall = recall[:recall.shape[0] - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#precision-thresholds curve:\npl.clf()\npl.plot(thresholds, precision, label='Precision-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Precision')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Precision-Thresholds Curve for BERT Fine Tuning Model\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recall-thresholds curve:\npl.clf()\npl.plot(thresholds, recall, label='Recall-Thresholds curve')\npl.xlabel('Thresholds')\npl.ylabel('Recall')\npl.ylim([0.0, 1.05])\npl.xlim([0.0, 1.0])\npl.title(\"Recall-Thresholds Curve for BERT Fine Tuning Model\")\npl.legend(loc=\"lower left\")\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_bert_classes = []\nfor i in range(test_pred_bert.shape[0]):\n    if test_pred_bert[i] >= 0.5:\n        test_pred_bert_classes.append(1)\n    else:\n        test_pred_bert_classes.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test.is_duplicate, test_pred_bert_classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualize the archetecture of models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file = 'model_plot.png', show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}